so today we are gonna be covering quite

a lot of materials so I will try not to

run but then yesterday young scooped me

completely so young talked about exactly

the same things I wanted to talk today

so I'm gonna go a bit faster please slow

me down if you actually are somehow lost

okay

so I will just try to be a little bit

faster than you sir

so today we are gonna be talking about

recurrent neural networks record neural

networks are one type of architecture we

can use in order to be to deal with

sequences of data what are sequences

what type of signal is a sequence

temporal is a temporal component but we

already seen data with temporal

component how what are they called

what dimensional what is the dimension

of that kind of signal so on the

convolutional net lesson we have seen

that a signal could be one this signal

to this signal 3d signal based on the

domain and the domain is what you map

from to go to right so temporal handling

sequential sequences of data is

basically dealing with one the data

because the domain is going to be just

the temporal axis nevertheless you can

also use RNN to deal with you know two

dimensional data you have double

Direction okay okay so this is a

classical neural network in the diagram

that is I'm used to draw where I

represent each in this case bunch of

neurons like each of those is a vector

and for example the X is my input vector

it's in pink as usual then I have my

hidden layer in a green in the center

then I have my final blue eared lane

layer which is the output network so

this is a three layer neural network in

my for my notation and so if some of you

are familiar with digital electronics

this is like talking about a

combinatorial logic your current output

depends only on the current input and

that's it there is no

there is no other input instead when we

are talking about our men we are gonna

be talking about something that looks

like this in this case our output here

on the right hand side depends on the

current input and on the state of the

system and again if you're a king of

digital electronics this is simply

sequential logic whereas you have an

internal state the onion is the

dimension flip-flop if you have no idea

what a flip-flop you know check it out

it's just some very basic memory unit in

digital electronics nevertheless this is

the only difference right in the first

case you have an output which is just

function of the input in the second case

you have an output which is function of

the input and the state of the system

okay that's the big difference yeah

vanilla is in American term for saying

it's plane doesn't have a taste that

American sorry I try to be the most

American I can in Italy you feel taken

an ice cream which is doesn't have a

taste it's gonna be fury laughter which

is milk taste in here we don't have milk

tests they have vanilla taste which is

the plain ice cream

okay Americans sorry

all right so oh so let's see what does

it change this with young representation

so young draws those kind of little

funky things here which represent a

mapping between a TENS tensor to another

painter from one a vector to another

vector right so there you have your

input vector X is gonna be mapped

through this item here to this hidden

representation so that actually

represent my fine transformation so my

rotation Plus this question then you

have the heater representation that you

have another rotation is question then

you get the final output right similarly

in the recurrent diagram you can have

these additional things this is a fine

transformation squashing that's like a

delay module with a final transformation

excursion and now you have the final one

affine transformation and squashing

right these things is making noise okay

sorry all right so what is the first

case first case is this one is a vector

to sequence so we input one bubble the

pink wonder and then you're gonna have

this evolution of the internal state of

the system the green one and then as the

state of the system evolves you can be

spitting out at every time stamp one

specific output what can be an example

of this kind of architecture so this one

could be the following my input is gonna

be one of these images and then the

output is going to be a sequence of

characters representing the English

description of whatever this input is so

for example in the center when we have a

herd of elephants so the last one herd

of elephants walking across a dry grass

field so it's very very very well

refined right then you have in the

center here for example two dogs play in

the in the grass maybe there are three

but okay they play they're playing in

the grass right so it's cool in this

case you know a red motorcycle park on

the side of the road

looks more pink or you know a little

blow a little a little girl in the pink

that is blowing bubbles that she's not

blowing right anything there all right

and then you also have you know even

more wrong examples right so you have

like yellow school bus parked in the

parking lot

well it's CL um but it's not a school

bus so it can be failing as well but I

also can do a very very nice you know

you can also perform very well so this

was from one input vector which is B for

example representation of my image to a

sequence of symbols which are D for

example characters or words that are

making here my English sentence okay

clear so far yeah okay another kind of

usage you can have is maybe the

following so you're gonna have sequence

two final vector okay so I don't care

about the intermediate sequences so okay

the top right is called Auto regressive

network and outer regressive network is

a network which is outputting an output

given that you feel as input the

previous output okay

so this is called Auto regressive you

have this kind of loopy part on the

network on the left hand side instead

I'm gonna be providing several sequences

yeah

that's gonna be the English translation

so you have a sequence of words that are

going to make up your final sentence

it's it's blue there you can think about

a index in a dictionary and then each

blue is going to tell you which word

you're gonna pick on an indexed

dictionary right so this is a school bus

right so oh yeah a yellow school bus you

go to a index of a then you have second

index you can figure out that is yellow

and then school box right so the

sequence here is going to be

representing the sequence of words the

model is out on the other side there on

the left you're gonna have instead I

keep feeding a sequence of symbols and

only at the end I'm gonna look what is

my final output what can be an

application of this one so something yun

also mentioned was different so let's

see if I can get my network to compile

Python or to an open pilot own

interpretation so in this case I have my

current input which I feed my network

which is going to be J equal 8580 for

then for X in range eight some - J 920

blah blah blah and then print this one

and then my network is going to be

tasked with the just you know giving me

twenty five thousand and eleven okay so

this is the final output of a program

and I enforced in the network to be able

to output me the correct output the

correct in your solution of this program

or even more complicated things for

example I can provide a sequence of

other symbols which are going to be

eighty eight thousand eight hundred

thirty seven then I have C is going to

be something then I have print this one

if something that is always true as the

other one and then you know the output

should be twelve thousand eight 184

right so you can train a neural net to

do these operations so you feed a

sequence of symbols and then at the

output you just enforce that the final

target should be a specific value okay

and these things making noise okay maybe

I'm better

all right so what's next next is going

to be for example a sequence to vector

to sequence this used to be the standard

way of performing length language

translation so you start with a sequence

of symbols here shown in pink so you

have a sequence of inputs then

everything gets condensed into this kind

of final age which is this H over here

which is going to be somehow my concept

right so I have a sentence I squeeze the

sentence temporal information into just

one vector which is representing the

meaning the message I'd like to send

across and then I get this meaning in

whatever representation unrolled back in

a different language right so I can

encode I don't know today I'm very happy

in English as a sequence of word and

then you know you can get LG Sonoma to

Felicia and then I speak outside

Thailand today or whatever now today I'm

very tired

Jin Chen walk han lei or whatever ok so

again you have some kind of encoding

then you have a compressed

representation and then you get like the

decoding given the same compressed

version ok and so for example I guess

language translation again recently we

have seen transformers and a lot of

things like in the recent time so we're

going to cover that the next lesson I

think but this used to be the state of

the art until few two years ago and here

you can see that if you actually check

if you do a PCA over the latent space

you have that words are grouped by

semantics ok so if we zoom in that

region there are we're gonna see that in

what in the same location you find all

the amounts december february november

whatever right if you put a few focus on

a different region you get that a few

days

next few miles and so on right so

different location will have some

specific you know common meaning so we

basically see in this case how by

training these networks you know just

with symbols they will pick up on some

specific semantics

you know features right in this case you

can see like there is a vector so the

vector that is connecting women to men

is gonna be the same vector that is well

woman - man which is this one I think is

gonna be equal to Queen - King right and

so yeah it's correct and so you're gonna

have that the same distance in this

embedding space will be applied to

things that are female and male for

example or in the other case you have

walk-in and walked swimming and swamp so

you always have this you know specific

linear transformation you can apply in

order to go from one type of word to the

other one or this one you have the

connection between cities and the

capitals all right so one more right I

think what's missing from the big

picture here it's a big picture because

it's so large no no it's such a big

picture because it's the overview okay

you didn't get the joke it's okay what's

missing here vector to seek with no okay

good but no because you can still use

the other one so you have this one the

vector is sequence to sequence right so

this one is you start feeding inside

inputs you start outputting something

right what can be an example of this

stuff so if you had a Nokia phone and

you use the t9 you know this stuff from

20 years ago you have basically

suggestions on what your typing is

you're typing right so this would be one

type of these suggestions where like one

type of this architecture as you getting

suggestions as you're typing things

through or you may have like speech to

captions right I talked and you have the

things

below or something very cool which is

the following so I start writing here

the rings of Saturn glitter while the

harsh ice two men look at each other hmm

okay they were enemies but the server

robots weren't okay okay hold on

so this network was trained on some

sci-fi novels and therefore you can just

type something then you let the network

start outputting some suggestions for

you so you know if you don't know how to

write a book then you can you know ask

your computer to help you out okay

that's so cool or one more that I really

like it this one is fantastic I think

you should read read it I think so you

put some kind of input there like the

scientist named alone what is it

or the prompt right so you put in the

the top prompt and then you get you know

this network start writing about very

interesting unicorns with multiple horns

is called horns say unicorn right okay

alright let's so cool just check it out

later and you can take a screenshot of

the screen anyhow so that was like the

eye candy such that you get you know

hungry now let's go into be PTT which is

the thing that they aren't really like

yesterday's PTT said okay alright let's

see how this stuff works okay so on the

left hand side we see again this vector

middle in the representation the output

to a fine transformation and then there

we have the classical equations right

all right so let's see how this stuff is

similar or not similar and you can't see

anything so for the next two seconds I

will want one minute I will turn off the

lights then I turn them on

[Music]

okay now you can see something all right

so let's see what are the questions of

this new architecture don't stand up

you're gonna be crushing yourself

alright so you have here the hidden

representation now there's gonna be this

nonlinear function of this rotation of a

stack version of my input which I

appended the previous configuration of

the hidden layer okay and so this is a

very nice compact notation it's just I

just put the two vectors one on top of

each other and then I sign assign I sum

the bias I also and define initial

condition my initial H is gonna be 0 so

at the beginning whenever I have T equal

1 this stuff is gonna be settle is a

vector of zeros and then I have this

matrix WH is gonna be two separate

matrices so sometimes you see this a

question is wh x times x plus w HH times

h t minus 1 but you can also figure out

that if you stock those two matrices you

know one attached to the other that you

just put this two vertical lines

completely equivalent notation but it

looked like very similar to whatever we

had here so hidden layer is affine

transformation of the input inner layer

is affine transformation of the input

and the previous value okay and then you

have the final output is going to be

again my final rotation so I'm gonna

turn on the light so no magic so far

right you're okay right you're with me

to shake the heads what about the others

no yes okay whatever

so this one is simply on the right hand

side I simply unroll over time such that

you can see how things are just not very

crazy like this loop here is not

actually a loop this is like a

connection to

next time steps right so that around

arrow means is just this right arrow so

this is a neural net it's dinkley a

neural net which is extended in in

length rather also not only in a in a

thickness right so you have a network

that is going this direction input and

output but as you can think as there's

been an extended input and this been an

extended output while all these

intermediate weights are all share right

so all of these weights are the same

weights and then you use this kind of

shared weights so it's similar to a

convolutional net in the sense that you

had this parameter sharing right across

different time domains because you

assume there is some kind of you know

stationarity right of the signal make

sense so this is a kind of convolution

right you can see how this is kind of a

convolution alright so that was kind of

you know a little bit of the theory we

already seen that so let's see how this

works for a practical example so in this

case we we are just reading this code

here so this is world language model you

can find it at the PI torch examples so

you have a sequence of symbols I have

just represented there every symbol is

like a letter in the alphabet and then

the first part is gonna be basically

splitting this one in this way right

so you preserve vertically in the time

domain but then I split the long long

long sequence such that I can now chop I

can use best bets bets how do you say

computation so the first thing you have

the best size is gonna be 4 in this case

and then I'm gonna be getting in my

first batch and then I will force the

network to be able to so this will be my

best back propagation through time

period and I will force the network to

output the next sequence of characters

ok so given that I have ABC I will force

my network to say D given that I have

GHI I will force the network to

come up with J givin em an orbit for

tonight poke at P and s tu for the

network 20 right so how can you actually

make sure you understand what I'm saying

whenever you are able to predict my next

world you're actually able to you know

you basically know in already what I'm

saying right yeah so by trying to

predict an upcoming word you're going to

be showing some kind of comprehension of

whatever is going to be this temporal

information in the data all right so

after we get the beds we have so how

does it work let's actually see you know

and about a bit of a detail this is

gonna be my first output is going to be

a batch with four items I feed this

inside the near corner all night and

then my neural net we come up with a

prediction of the upcoming sample right

and I will force that one to be my B H

and T okay then I'm gonna be having my

second input I will provide the previous

hidden state to the current RNN I will

feel these inside and then I expect to

get the second line of the output the

target right and then so on right I get

the next state and sorry the next input

I get the next state and then I'm gonna

get inside the neural net the RN n I

which I will try to force to get the

final target okay so far yeah

each one is gonna be the output of the

internet recurrent neural net right I'll

show you the equation before you have h1

comes out from this one right second the

output I'm gonna be forcing the output

actually to be my target my next word in

the sequence of letters right so I have

a sequence of words force my network to

predict what's the next word given the

previous word know h1 is going to be fed

inside here and you stuck the next word

the next word together with the previous

state and then you'll do a rotation of

the previous word with a previous sorry

the new word with the next state the new

word with the previous state you'll do

our rotation here find transformation

right and then you apply the

non-linearity so you always get a new

word that is the current X and then you

get the previous state just to see in

what state the system once and then you

output a new output right and so we are

in this situation here we have a bunch

of inputs I have my first input and then

I get the first output I have this

internal memory that is sent forward and

then this network will now be aware of

what happened here and then I input the

next input and so on I get the next

output and I force the output to be the

output here the value inside the batch

ok alright what's missing now

[Music]

this is for PowerPoint drawing

constraint all right what's happening

now so here I'm gonna be sending the

here I just drawn an arrow with the

final HT but there is a slash on the

arrow

what is the slash on the arrow who can

understand what the slash mean of course

there will be there is gonna be the next

batch they're gonna be starting from

here D and so on this is gonna be my

next batch d j pv e KQ v + FL rx right

so this slash here means do not back

propagate through okay so that one is

gonna be calling dot detach in Porsche

which is gonna be stopping the gradient

to be you know propagated back to

forever okay so this one say know that

and so whenever I get the sorry no no

gradient such that when I input the next

gradient the first input here it's gonna

be this guy over here and also of course

without gradient such that we don't have

an infinite length RN n okay make sense

yes

no I assume it's a yes

okay so vanishing and exploding

gradients we touch them upon these also

yesterday so again I'm kind of going a

little bit faster to the intent user so

let's see how this works

so usually for our recurrent neural

network you have an input you have a

hidden layer and then you have an output

then this value of here how do you get

this information through here what what

what does this R represent do you

remember the equation of the hidden

layer so the new hidden layer is gonna

be the previous hidden layer which we

rotate

alright so we rotate the previous hidden

layer and so how do you rotate hidden

layers matrices right and so every time

you see all ads on tile arrow there is a

rotation there is a matrix

now if the you know this matrix can

change the sizing of your final output

right so if you think about perhaps

let's say the determinant right if the

terminal is unitary it's a mapping the

same areas for the same area if it's

larger than one they're going to be

getting you know this radians to getting

larger and larger or if it's smaller

than I'm gonna get these gradients to go

to zero whenever you perform the back

propagation in this direction okay so

the problem here is that whenever we do

is send gradients back so the gains are

going to be going down like that are

gonna be going like down like this then

down like this way and down like this

way and also all down this way and so on

right so the gradients are going to be

always going against the direction of

the arrow in H ro has a matrix inside

right and again this matrix will affect

how these gradients propagate and that's

why you can see here although we have a

very bright input that one like gets

lost through oh well if you have like a

gradient coming down here the gradient

gets you know kill over time okay so how

do we fix that to fix this one we simply

remove the matrices in this horizontal

operation does it make sense no yes no

the problem is that the next hidden

state will have you know its own input

memory coming from the previous step

through a matrix multiplication now this

matrix multiplication will affect what's

gonna be the gradient that comes in the

other direction okay

so whenever you have an output here you

have a final loss now you have the grade

that are gonna be going against the

arrows up to the input the problem is

that this gradient which is going

through the in the opposite direction of

these arrows will be multiplied by the

matrix right the transpose of the matrix

and again these matrices will affect

what is the overall norm of this

gradient right and it will be all

killing it

you have vanishing gradient or you're

gonna have exploding the gradient which

is going to be whenever is going to be

getting amplified right so in order to

be avoiding that we have to avoid so you

can see this is a very deep network so

recurrently our network where the first

deep networks back in the night is

actually and the word

depth was actually in time which and of

course they were facing the same issues

we face with deep learning in modern day

days where ever we were still like

stacking several layers we were

observing that the gradients get lost as

depth right so how do we solve gradient

getting lost through the depth in a

current days

skipping constant connection right the

receiver connections we use and

similarly here we can use skip

connections as well when we go down well

up in in time okay so let's see how this

works

yeah so the problem is that the

gradients are only going in the backward

paths right back

[Music]

well the gradient has to go the same way

it went forward by the opposite

direction right I mean you're computing

chain rule so if you have a function of

a function of a function then you just

use those functions to go back right the

point is that whenever you have these

gradients coming back they will not have

to go through matrices therefore also

the forward part has not doesn't have to

go through the matrices meaning that the

memory cannot go through matrix

multiplication if you don't want to have

this effect when you perform back

propagation okay

yeah it's gonna be worth much better

working I show you in the next slide

[Music]

show you next slide

so how do we fix this problem well

instead of using one recurrent neural

network we're gonna using for recurrent

neural network okay so the first

recurrent Network alcohol on the first

network is gonna be the one that goes

from the input to this intermediate

state then I have other three networks

and each of those are represented by

these three symbols 1 2 & 3

okay think about this as our open mouth

and it's like a closed mouth okay like

the emoji okay so if you use this kind

of for net for recurrent neural network

be regular Network you gotta have for

example from the input I send things

through in the open mouth therefore it

gets here I have a closed mouth here so

nothing goes forward then I'm gonna have

this open mouth here such that the

history goes forward so the history gets

sent forward without going through a

neural network matrix multiplication

it just gets through our open mouth and

all the other inputs find a closed mouth

so the hidden state will not change upon

new inputs okay and then here you're

gonna have a open mouth here such that

you can get the final output here then

the open mouth keeps going here such

that you have another output there and

then finally you get the last closed

mouth at the last one now if you perform

back prop you will have the gradients

flowing through the open mouth and you

don't get any kind of matrix

multiplication so now let's figure out

how these open mouths are represented

how are they instantiated in like in in

terms of mathematics is it clear design

right so now we are using open and

closed mouths and each of those mouths

is plus the the first guy here that

connects the input to the hidden are

brn ends so these on here that is a

gated recurrent network it's simply for

normal recurrent neural network combined

in a clever way such that you have

multiplicative interaction and not

matrix interaction is it clear so far

this is like intuition I haven't shown

you how all right so let's figure out

who made this and how it works okay so

we're gonna see now those long short

term memory or gated recurrent neural

networks so I'm sorry okay that was the

dude okay this is the guy who actually

invented this stuff actually him and his

students back some in 1997 and we were

drinking here together okay all right so

that is the question of a recurrent

neural network and on the top left are

you gonna see in the diagram so I just

make a very compact version of this

recurrent neural network here is going

to be the collection of equations that

are expressed in a long short term

memory they look a little bit dense so I

just draw it for you here okay let's

actually goes through how this stuff

works

so I'm gonna be drawing an interactive

animation here so you have your input

gate here which is going to be an affine

transformation so all of these are

recurrent Network write the same

equation I show you here so this input

transformation will be multiplying my C

tilde which is my candidate gate here I

have a don't forget gate which is

multiplying my previous value of my cell

memory and then my Poppa stylist maybe

don't forget previous plus input ii i'm

gonna show you now how it works then i

have my final hidden representations to

be multiplication element wise

multiplication between my output gate

and my you know whatever hyperbolic

tangent version of the cell such that

things are bounded and then I have

finally my C tilde which is my candidate

gate is simply

Anette right so you have one recurrent

network one that modulates the output

one that modulates this is don't forget

gate and this is the input gate

so all this interaction between the

memory and the gates is a multiplicative

interaction and this forget input and

don't forget the input and output are

all sigmoids and therefore they are

going from 0 to 1 so I can multiply by a

0 you have a closed mouth or you can

multiply by 1 if it's open mouth right

if you think about being having our

internal linear volume which is below

minus 5 or above plus 5 okay such that

you using the you use the gate in the

saturated area or 0 or 1 right you know

the sigmoid so let's see how this stuff

works

this is the output let's turn off the

output how do I do

turn off the output I simply put a 0

inside so let's say I have a purple

internal representation see I put a 0

there in the output gate the output is

going to be multiplying a 0 with

something you get 0 okay then let's say

I have a green one I have one then I

multiply one with the purple I get

purple and then finally I get the same

value similarly I can control the memory

and I can for example we set it in this

case I'm gonna be I have my internal

memory see this is purple and then I

have here my previous guy which is gonna

be blue I guess I have a zero here and

therefore the multiplication gives me a

zero there I have here a zero so

multiplication is gonna be giving a zero

at some two zeros and I get a zero

inside of memory so I just erase the

memory and you get the zero there

otherwise I can keep the memory I still

do the internal thing I did a new one

but I keep a wonder such that the

multiplication gets blue the Sun gets

blue and then I keep sending out my

bloom finally I can write such that I

can get a 1 in the input gate the

multiplication gets purple then the I

set a zero in the don't forget such that

the

we forget and then multiplication gives

me zero I some do I get purple and then

I get the final purple output okay so

here we control how to send how to write

in memory how to reset the memory and

how to output something okay so we have

all different operation this looks like

a computer - and in an yeah it is

assumed in this case to show you like

how the logic works as we are like

having a value inside the sigmoid has

been or below minus 5 or being above

plus 5 such that we are working as a

switch 0 1 switch okay the network can

choose to use this kind of operation to

me make sense I believe this is the

rationale behind how this network has

been put together the network can decide

to do anything it wants usually they do

whatever they want but this seems like

they can work at least if they've had to

saturate the gates it looks like things

can work pretty well so in the remaining

15 minutes of kind of I'm gonna be

showing you two notebooks I kind of went

a little bit faster because again there

is much more to be seen here in the

notebooks so yeah

so this the the actual weight the actual

gradient you care here is gonna be the

gradient with respect to previous C's

right the thing you care is gonna be

basically the partial derivative of the

current seen with respect to previous

C's such that you if you have the

original initial C here and you have

multiple C over time you want to change

something in the original C you still

have the gradient coming down all the

way until the first C which comes down

to getting gradients through that matrix

WC here right so if you want to change

those weights here you just go through

the chain of multiplications that are

not involving any matrix multiplication

as such that you when you get the

gradient it still gets multiplied by one

all the time and it gets down to

whatever we want to do okay did I answer

your question

so the matrices will change the

amplitude of your gradient right so if

you have like these largest eigenvalue

being you know 0.0001 every time you

multiply you get the norm of this vector

getting killed right so you have like an

exponential decay in this case if my

forget gate is actually always equal to

1 then you get C equal C minus team

derivative what is the partial

derivative of C T with respect to what

is the partial derivative of C T with

respect to C minus t minus 1 1 right so

the parts of the relative that is the

thing that you actually multiply every

time there's gonna be 1 so output

gradient output gradients can be input

gradients right yeah i'll pavillions

gonna be implicit because it would apply

the output gradient by the derivative of

this module right if the

this module is e1 then the thing that is

here keeps going that is the rationale

behind this now this is just for drawing

purposes I assumed it's like a switch

okay such that I can make things you

know you have a switch on and off to

show like how it should be working maybe

doesn't work like that but still it

works it can work this way right yeah so

that's the implementation of pro

question is gonna be simply you just pad

all the other sync when sees with zeros

before the sequence so if you have

several several sequences yes several

sequences that are of a different length

you just put them all aligned to the

right

and then you put some zeros here okay

such that you always have in the last

column the latest element if you put two

zeros here it's gonna be a mess in right

in the code if you put the zeros in the

in the beginning you just stop doing

back propagation when you hit the last

symbol right so you start from here you

go back here so you go forward then you

go back prop and stop whenever you

actually reach the end of your sequence

if you pad on the other side you get a

bunch of drop there in the next ten

minutes so you're gonna be seen two

notebooks if you don't have other

questions okay wow you're so quiet okay

so we're gonna be going now for sequence

classification alright so in this case

I'm gonna be I just really stuff loud

out loud the goal is to classify a

sequence of elements sequence elements

and targets are represented locally

input vectors with only one nonzero bit

so it's a one hot encoding the sequence

starts with a B for beginning and end

efore end and otherwise consists of a

randomly chosen symbols from a set a b c

and d which are some kind of noise

expect for two elements in position t1

and t2 this position can be either or X

or Y in for the hard difficulty level

you have for example that the sequence

length length is chose randomly between

100 and 110 10 t1 is randomly chosen

between 10 and 20 Tinto is randomly

chosen between 50 and 60 there are four

sequences classes QRS nu which depends

on the temporal order of x and y so if

you have X X you can be getting a Q x

and y you get an R Y and X you get an S

and YY with you so we're going to be

doing a sequence classification based on

the X and y or whatever those to import

to these kind of triggers okay

and in the middle in the middle you can

have ABCD in random positions like you

know randomly generated is it clear so

far so we do cast a classification of

sequences where you may have these x and

y's or X X or Y or Y X so in this case

I'm showing you first the first input so

the return type is a tuple of sequence

of two which is going to be what is the

output of this example generator and so

let's see what is what is this thing

here so this is my data I'm going to be

feeding to the network so I have one two

three four five six seven eight there

are eight different symbols here in a

row every time why there are eight we

have x and y a b c and d beginning and

end right so we have one hot out of you

know eight characters and then i have a

sequence of rows which are my sequence

of symbols okay in this case you can see

here i have a beginning with all zeros

why is all zeros padding right so in

this case the sequence was shorter than

the expect the maximum sequence in the

bed

and then the first first sequence has an

extra zero item at the beginning in them

you're gonna have like in this case the

second item is of the two a pole to pole

is the corresponding best class for

example I have a batch size of 32 and

then I'm gonna have an output size of 4

y 4 Q R whatever right the QRS you okay

so I have 4 a 4 dimensional target

vector and I have a sequence of 8

dimensional vectors as input okay so

let's see how this sequence looks like

in this case is gonna be the beginning

BX CX CBE so X X let's see X X X X is Q

right so we have our Q sequence and

that's why the final target is a Q the 1

0 0 0 and then you're gonna see B B X C

so the second item and the second last

is gonna be B lowercase B you can see

here the second item and the second last

item is going to be a be okay all right

so let's now create a recurrent Network

in a very quick way so here I can simply

say my recurrent network is going to be

torch and an RNN and I'm gonna be using

a reader network really non-linearity

and then I have my final linear layer in

the other case I'm gonna be using a led

STM and then I'm gonna have a final

inner layer so I just execute these guys

I have my training loop and I'm gonna be

training for 10 books so in the training

group you can be always looking for

those five different steps first step is

gonna be get the data inside the model

right so that's step number one what is

step number two there are five steps we

remember hello

you feel that you feed the network if

you feed the network with some data then

what do you do you compute the loss okay

then we have compute step to compute the

loss fantastic number three is zero the

cash right then number four which is

computing the off yes lost dog backwards

lost not backward don't compute the

partial derivative of the loss with

respect to the network's parameters yeah

here backward finally number five which

is step in opposite direction of the

gradient okay all right those are the

five steps you always want to see in any

training blueprint if someone is missing

then you're up okay so we try now

the RNN and the lsdm and you get

something looks like this

so our NN goes up to 50% in accuracy and

then the lsdm got 100% okay oh okay

first of all how many weights does this

lsdm have compared to the RN n four

times more weights right so it's not a

fair comparison I would say because lsdm

is simply for rnns combined somehow

right so this is a two layer neural

network whereas the other one is at one

layer right always both ever like it has

one hidden layer they are an end if

Alice TM we can think about having two

hidden so again one layer two layers

well one hidden to lead in one set of

parameters four sets of the same numbers

like okay not fair okay anyway

let's go with hundred iterations okay so

now I just go with 100 iterations and I

show you how if they work or not and

also when I be just clicking things such

that we have time to go through stuff

okay now my computer's going to be

complaining all right so again what are

the five types of operations like five

okay now is already done

sorry I was going to do okay so this is

the RNN right RNN and finally actually

gave to 100% okay so iron and it just

let it more time like a little bit more

longer training actually works the other

one okay and here you can see that we

got 100% in twenty eight bucks okay the

other case we got 2,100 percent in

roughly twice as long

twice longer at a time okay so let's

first see how they perform here so I

have this sequence BC y dy Dae which is

au sequence and then we ask the network

and he actually meant for actually like

labels it as you okay so below we're

gonna be seeing something very cute so

in this case we were using sequences

that are very very very very small right

so even the RNN is able to train on

these small sequences so what is the

point of using a lsdm well we can first

of all increase the difficulty of the

training part and we're gonna see that

the RN ends can be miserably failing

whereas the lsdm keeps working in this

visualization part below okay I train a

network now Alice and lsdm now with the

moderate level which has eighty symbols

rather than eight or ten ten symbols so

you can see here how this model actually

managed to succeed at the end although

there is like a very big spike and I'm

gonna be now drawing the value of the

cell state over time okay so I'm going

to be input in our sequence of eighty

symbols and I'm gonna be showing you

what is the value of the hidden state

hidden State

so in this case I'm gonna be showing you

[Music]

hidden hold on

yeah I'm gonna be showing I'm gonna send

my input through a hyperbolic tangent

such that if you're below minus 2.5 I'm

gonna be mapping to minus 1 if you're

above 2.5 you get mapped to plus 1 more

or less and so let's see how this stuff

looks so in this case here you can see

that this specific hidden layer picked

on the X here and then it became red

until you got the other X right so this

is visualizing the internal state of the

LSD and so you can see that in specific

unit because in this case I use hidden

representation like hidden dimension of

10 and so in this case the 1 2 3 4 5 the

fifth hidden unit of the cell lay the

5th cell actually is trigger by

observing the first X and then it goes

quiet after seen the other acts this

allows me to basically you know take

care of I mean recognize if the sequence

is ru OPRS okay does it make sense okay

oh this one more notebook I'm gonna be

showing just quickly which is the Eco

data in this case I'm gonna be in South

corner I'm gonna have a network echo in

whatever I'm saying so if I say

something I asked a network to say if I

say something I asked my neighbor to say

if I say something I ask ok Anderson

right ok so in this case here and I'll

be inputting this is the first sequence

is going to be 0 1 1 1 1 0 and you'll

have the same one here 0 1 1 1 1 0 and I

have 1 0 1 1 0 1 etc right so in this

case if you want to output something

after some

right this in this case is three time

step after you have to have some kind of

short-term memory where you keep in mind

what I just said where you keep in mind

what I just said where you keep in mind

[Music]

what I just said yeah that's correct so

you know pirating actually requires

having some kind of working memory

whereas the other one the language model

which it was prompted prompted to say

something that I haven't already said

right so that was a different kind of

task you actually had to predict what is

the most likely next word in keynote you

cannot be always right right but this

one you can always be right you know

this is there is no random stuff anyhow

so I have my first batch here and then

the sec the white patch which is the

same similar thing which is shifted over

time and then we have we have to chunk

this long long long sequence so before I

was sending a whole sequence inside the

network and I was enforcing the final

target to be something right in this

case I had to chunk if the sequence goes

this direction I had to chunk my long

sequence in little chunks and then you

have to fill the first chunk keep trace

of whatever is the hidden state send a

new chunk where you feed and initially

as the initial hidden state the output

of this chant right so you feed this

chunk you have a final hidden state then

you feed this chunk and as you put you

have to put these two as input to the

internal memory right now you feed the

next chunk where you put this one as

input as to the internal state and you

we are going to be comparing here RNN

with analyst TMS I think so at the end

here you can see that okay we managed to

actually get we are an n/a accuracy that

goes 100 100 percent then if you are

starting now to mess with the size of

the memory chunk with a memory interval

you can be seen

with the lsdm you can keep this memory

for a long time as long as you have

enough capacity the RNN after you reach

some kind of length you start forgetting

what happened in the past and it was

pretty much everything for today so stay

warm wash your hands and I'll see you

next week bye bye

[Music]

